[{"title":"hexo博客文章加密","url":"/2024/01/29/hexo博客文章加密/","content":"\n静态博客加密插件：[hexo-blog-encrypt](https://www.npmjs.com/package/hexo-blog-encrypt)，搭配此插件你可以写一些私密博客，通过密码验证的方式让人不能随意浏览。\n\n### 安装：\n\n```shell\nnpm install hexo-blog-encrtpt\n```\n\n### 用法：\n\n文章头添加关键字<font color=red>password</font>即可设置文章访问密码\n\n```c\n---\ntitle: Hello World\ndate: 2016-03-30 21:18:02\npassword: hello\n---\n```\n\n高级用法：\n\n```c\n---\ntitle: Hello World\ndate: 2016-03-30 21:12:21\npassword: hello\nabstract: Here's something encrypted, password is required to continue reading.\nmessage: Hey, password is required here.\nwrong_pass_message: Oh, this is an invalid password. Check and try again, please.\n---\n```\n\n- `abstract`：用于设置加密文章显示的摘要。\n- `message`：输入密码提示。\n- `wrong_pass_message`: 输入错误提示。\n","tags":["hexo"]},{"title":"HPCGame记录","url":"/2024/01/23/HPCGame记录/","content":"\n前言：刚打完ASC，回家本来想直接摆烂的，看到HPCGame心里痒痒，不打真觉得难受\n\n## A.欢迎参赛！\n\n这个题就是说一些比赛的相关内容，直接提交后就会返回提供的集群账号密码\n\n## B.流量席卷土豆\n\n  [流量席卷土豆.mhtml](流量席卷土豆.mhtml) \n\n这个题挺有意思的，刚开始看到题的时候说实话我人有点慌了(好多字)冷静下来认真读题后发现，就是一个作业提交和软件使用而已。\n\n**解题步骤如下：**\n\n1、srun提交命令提取 SSH 流量：\n\n（赛题中提供的路径有问题，找了一下还有一层子目录pcaps）\n\n```\nsrun -p C064M0256G -N4 --ntasks-per-node=4 bash -c 'tshark -r /lustre/shared_data/potato_kingdom_univ_trad_cluster/pcaps/$SLURM_PROCID.pcap -Y ssh -w $SLURM_PROCID.ssh.pcap'\n```\n\n当前路径就会得到如下文件\n\n![image-20240123134122253](HPCGame记录/image-20240123134122253.png)\n\n2、合并 PCAP 文件：\n\n```\nmergecap -w merged.pcap *.ssh.pcap\n```\n\n![image-20240123134322241](HPCGame记录/image-20240123134322241.png)\n\n3、破解密码：\n\n```\nquantum-cracker merged.pcap\n```\n\n![image-20240123134517990](HPCGame记录/image-20240123134517990.png)\n\n4、获取 Slurm JobID\n\n```\nsacct -u $(whoami) --format JobID | tail\n```\n\n![image-20240123134612339](HPCGame记录/image-20240123134612339.png)\n\n取最后一个的整数，提交即可。\n\n## C.简单的编译\n\n [简单编译.mhtml](简单编译.mhtml) \n\n这道题就是写Makefile和CMakeLists.txt\n\n```\n在本题中，你需要写一个简单的Makefile文件，和一个简单的CMakeLists.txt文件，来编译 Handout 中所提供的三个简单程序。\n\n其中，hello_cuda.cu是一个简单的cuda程序，hello_mpi.cpp是一个简单的mpi程序，hello_omp.cpp是一个简单的 OpenMP 程序。它们都做了同一个简单的事情：从文件中读取一个向量并求和。\n\n你需要上传Makefile和CMakeLists.txt文件。我们会根据以下策略来评测你所写的配置文件的正确性。\n\n对于Makefile文件，我们会在项目根目录下执行make命令。然后在项目根目录下检查程序是否被生成，并运行以检测正确性。\n对于CMakeLists.txt文件，我们会在项目根目录下执行mkdir build; cd build; cmake ..; make。然后我们会在build目录下检查程序是否被正确生成，并运行以检测正确性。\n对于所有类型的文件，hello_cuda.cu所编译出的文件名应为hello_cuda；hello_mpi.cpp所编译出的文件名应为hello_mpi；hello_omp.cpp所编译出的文件名应为hello_omp。\n```\n\n集群GPU节点连不上，用记事板手搓了一下，提交上去Makefile一直报错，\n\n查了一下是tab缩进的问题，后来在终端里面用vim编写，复制，完美解决\n\n（长记性了，用vim来编辑才能正常识别tab）\n\n```makefile\n.PHONY: all clean\n\nall: hello_cuda hello_mpi hello_omp\n\nhello_cuda: hello_cuda.cu\n\tnvcc -o hello_cuda hello_cuda.cu\n\nhello_mpi: hello_mpi.cpp\n\tmpic++ -o hello_mpi hello_mpi.cpp\n\nhello_omp: hello_omp.cpp\n\tg++ -fopenmp -o hello_omp hello_omp.cpp\n\nclean:\n\trm -f hello_cuda hello_mpi hello_omp\n```\n\n```cmake\ncmake_minimum_required(VERSION 3.10)\nproject(test)\n\nenable_language(CUDA)\nadd_executable(hello_cuda hello_cuda.cu)\n\nfind_package(MPI REQUIRED)\nadd_executable(hello_mpi hello_mpi.cpp)\ntarget_link_libraries(hello_mpi MPI::MPI_CXX)\n\nfind_package(OpenMP REQUIRED)\nadd_executable(hello_omp hello_omp.cpp)\ntarget_link_libraries(hello_omp OpenMP::OpenMP_CXX)\n```\n\n## D.小北问答 Classic\n\n [小北问答 Classic.mhtml](小北问答.mhtml) \n\n![image-20240123135107256](HPCGame记录/image-20240123135107256.png)\n\n```\n答案：{\"t1\":\"65.396\",\"t2.1\":\"105.26\",\"t2.2\":\"107.14\",\"t3\":\"iv. GCC\",\"t4.1\":\"ii. 进程\",\"t4.2\":\"i. 线程\",\"t4.3\":\"i. 线程\",\"t5.1\":\"8\",\"t5.2\":\"i. 在 CPU 中设计独立的 AVX512 运算单元，但有可能导致调用 AVX512 指令集时因功耗过大而降频\",\"t5.3\":\"ii. 复用两个 AVX2 运算单元执行 AVX512 运算\",\"t6\":\"iv. OpenGL\",\"t8\":\"iv. PCIe\",\"t9\":\"ii. Slurm\",\"t10\":\"i. 缓存未命中\",\"t7\":\"iii,iv\"}\n```\n\n## E.齐心协力\n\n [齐心协力.mhtml](齐心协力.mhtml) \n\n这道题卡了两天，Ray集群老是出错，后来偶然交了一发过了\n\n后来发现提示里有放置组的概念，学习了一下，改了一下代码，111.6/120分，满足了\n\n```python\nimport ray\nimport numpy as np\nimport os\n\n@ray.remote(num_cpus=1)\nclass Counter:\n    def __init__(self, flag_index):\n        self.weight_matrix = np.load(f\"weights/weight_{flag_index}.npy\")\n    \n    def process(self, input_matrix):\n        return np.maximum(0, np.dot(input_matrix, self.weight_matrix))\n\nif __name__ == \"__main__\":\n    ray.init(address=os.environ['RAY_CLUSTER_ADDR'])\n\n    # 创建放置组，每组有 4 个 CPU\n    placement_groups = [ray.util.placement_group([{\"CPU\": 4}], strategy=\"STRICT_PACK\") for _ in range(4)]\n    ray.get([pg.ready() for pg in placement_groups])  # 等待放置组就绪\n\n    if not os.path.exists(\"outputs\"):\n        os.makedirs(\"outputs\")\n\n    # 在每个放置组中创建 Counter 实例\n    counters = [[Counter.options(placement_group=placement_groups[i], placement_group_bundle_index=0).remote(j) for j in range(4)] for i in range(4)]\n\n    output_results = []\n    for i in range(100):\n        input_matrix = np.load(f\"inputs/input_{i}.npy\")\n        result = input_matrix\n        for counter in counters[i % 4]:\n            result = counter.process.remote(result)\n        output_results.append(result)\n\n    # 收集和保存结果\n    for i, result in enumerate(output_results):\n        output = ray.get(result)\n        np.save(f\"outputs/output_{i}.npy\", output)\n```\n\n\n\n## F.高性能数据校验\n\n [高性能数据校验.mhtml](高性能数据校验.mhtml) \n\n这道题一开始随便开了个MPI竟然就有80多分，而且我题都没认真看，后来随便改改卡到了90（没错，是卡bug到了90）题目重测后直接回归0分，着急！\n\n仔细读了一下题，看到提升里的SHA512函数，就想着用SHA512替换原来的一堆函数（没有认真去看那几个函数），然后就陷入了死循环里，SHA512没法进行并行计算，第i个块的计算依赖于第i-1个。\n\n后来偶然的仔细看了一下代码，发baseline代码中的函数完全解决了上面的问题，可以拆开做，只需要用更多的数组来记录一下就行了![image-20240125213452456](HPCGame记录/image-20240125213452456.png)\n\n修改了一下代码：\n\n```cpp\nvoid checksum(uint8_t *data, size_t len, uint8_t *obuf) {\n  int num_block = (len + BLOCK_SIZE - 1) / BLOCK_SIZE;\n  \n  EVP_MD *sha512 = EVP_MD_fetch(nullptr, \"SHA512\", nullptr);\n  EVP_MD_CTX *ctx[num_block];\n  for(int i=0; i<num_block; i++)//多创建一点ctx让记录数据和校验位分开计算\n  {\n    ctx[i]=EVP_MD_CTX_new();\n    EVP_DigestInit_ex(ctx[i], sha512, nullptr);\n  }\n  uint8_t prev_md[SHA512_DIGEST_LENGTH];\n  SHA512(nullptr, 0, prev_md);\n  for (int i = 0; i < num_block; i++) {\n    uint8_t buffer[BLOCK_SIZE]{};\n    EVP_DigestInit_ex(ctx[i], sha512, nullptr);\n    std::memcpy(buffer, data + i * BLOCK_SIZE, std::min(BLOCK_SIZE, len - i * BLOCK_SIZE));\n    EVP_DigestUpdate(ctx[i], buffer, BLOCK_SIZE);\n  }\n  for(int i=0;i<num_block;i++)\n  {\n    EVP_DigestUpdate(ctx[i], prev_md, SHA512_DIGEST_LENGTH);\n    unsigned int len = 0;\n    EVP_DigestFinal_ex(ctx[i], prev_md, &len);\n    EVP_MD_CTX_free(ctx[i]);\n  }\n  std::memcpy(obuf, prev_md, SHA512_DIGEST_LENGTH);\n  EVP_MD_free(sha512);\n}\n```\n\n就这一改，run了一下，成了！思路正确！测试了一下几个for循环的耗时，中间数据处理的循环占比最高![image-20240125213808760](HPCGame记录/image-20240125213808760.png)\n\n那就没问题了，开始修改MPI并行程序：\n\n```cpp\n#include <algorithm>\n#include <chrono>\n#include <cstring>\n#include <filesystem>\n#include <fstream>\n#include <iostream>\n#include <mpi.h>\n#include <openssl/evp.h>\n#include <openssl/sha.h>\n\nnamespace fs = std::filesystem;\n\nconstexpr size_t BLOCK_SIZE = 1024 * 1024;\n\nvoid print_checksum(std::ostream &os, uint8_t *md, size_t len);\n\nint main(int argc, char *argv[]) {\n\n  MPI_Init(&argc, &argv);\n\n  int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  if (rank == 0) {\n    if (argc < 3) {\n      std::cout << \"Usage: \" << argv[0] << \" <input_file> <output_file>\"\n                << std::endl;\n      MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n  }\n    fs::path input_path = argv[1];\n    fs::path output_path = argv[2];\n\n    auto total_begin_time = std::chrono::high_resolution_clock::now();\n\n    auto file_size = fs::file_size(input_path);\n    std::cout << input_path << \" size: \" << file_size << std::endl;\n    int num_block = (file_size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    int proc_num_block = num_block / 8;\n    auto len = file_size / 8;\n    uint8_t *buffer = nullptr;\n    if (file_size != 0) {\n      buffer = new uint8_t[len];\n      std::ifstream istrm(input_path, std::ios::binary);\n      istrm.seekg(rank*len);\n      istrm.read(reinterpret_cast<char *>(buffer), len);\n    }\n\n    auto begin_time = std::chrono::high_resolution_clock::now();\n    \n    EVP_MD *sha512 = EVP_MD_fetch(nullptr, \"SHA512\", nullptr);\n    EVP_MD_CTX *ctx[proc_num_block];\n    for(int i=0; i<proc_num_block; i++)\n    {\n      ctx[i]=EVP_MD_CTX_new();\n      EVP_DigestInit_ex(ctx[i], sha512, nullptr);\n    }\n\n    uint8_t prev_md[SHA512_DIGEST_LENGTH];\n    \n    for (int i = 0; i < proc_num_block; i++) {\n      uint8_t buffer_temp[BLOCK_SIZE]{};\n      std::memcpy(buffer_temp, buffer + i * BLOCK_SIZE, std::min(BLOCK_SIZE, len - i * BLOCK_SIZE));\n      EVP_DigestUpdate(ctx[i], buffer_temp, BLOCK_SIZE);\n    }\n    delete[] buffer;\n\n    if(rank==0){\n      SHA512(nullptr, 0, prev_md);\n      for(int i=0;i<proc_num_block;i++)\n      {\n        EVP_DigestUpdate(ctx[i], prev_md, SHA512_DIGEST_LENGTH);\n        unsigned int len_ex = 0;\n        EVP_DigestFinal_ex(ctx[i], prev_md, &len_ex);\n        EVP_MD_CTX_free(ctx[i]);\n      }\n      MPI_Send(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, 1, 0, MPI_COMM_WORLD);\n    }else if(rank>0 && rank<7)\n    {\n      MPI_Recv(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int i=0;i<proc_num_block;i++)\n      {\n        EVP_DigestUpdate(ctx[i], prev_md, SHA512_DIGEST_LENGTH);\n        unsigned int len_ex = 0;\n        EVP_DigestFinal_ex(ctx[i], prev_md, &len_ex);\n        EVP_MD_CTX_free(ctx[i]);\n      }\n      MPI_Send(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, rank + 1, 0, MPI_COMM_WORLD);      \n    }else if(rank==7){\n      MPI_Recv(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int i=0;i<proc_num_block;i++)\n      {\n        EVP_DigestUpdate(ctx[i], prev_md, SHA512_DIGEST_LENGTH);\n        unsigned int len_ex = 0;\n        EVP_DigestFinal_ex(ctx[i], prev_md, &len_ex);\n        EVP_MD_CTX_free(ctx[i]);\n      }\n    }\n\n    EVP_MD_free(sha512);\n  \n  if(rank==7)\n  {\n    auto end_time = std::chrono::high_resolution_clock::now();\n\n    // print debug information\n    std::cout << \"checksum: \";\n    print_checksum(std::cout, prev_md, SHA512_DIGEST_LENGTH);\n    std::cout << std::endl;\n\n    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(\n        end_time - begin_time);\n\n    std::cout << \"checksum time cost: \" << std::dec << duration.count() << \"ms\"\n              << std::endl;\n\n    // write checksum to output file\n    std::ofstream output_file(output_path);\n\n    print_checksum(output_file, prev_md, SHA512_DIGEST_LENGTH);\n  \n    auto total_end_time = std::chrono::high_resolution_clock::now();\n\n    auto total_duration = std::chrono::duration_cast<std::chrono::milliseconds>(\n        total_end_time - total_begin_time);\n\n    std::cout << \"total time cost: \" << total_duration.count() << \"ms\"\n              << std::endl;\n  \n  }\n\n  MPI_Finalize();\n  return 0;\n}\n\nvoid print_checksum(std::ostream &os, uint8_t *md, size_t len) {\n  for (int i = 0; i < len; i++) {\n    os << std::setw(2) << std::setfill('0') << std::hex\n       << static_cast<int>(md[i]);\n  }\n}\n```\n\n效果不算特别好，90/120分，检查了一下，发现读取占用时间太长，\n\n偶然想到可以不用一次性读入全部数据，让IO和CPU形成流水线即可\n\n```c\n#include <algorithm>\n#include <chrono>\n#include <cstring>\n#include <filesystem>\n#include <fstream>\n#include <iostream>\n#include <mpi.h>\n#include <openssl/evp.h>\n#include <openssl/sha.h>\n\nnamespace fs = std::filesystem;\n\nconstexpr size_t BLOCK_SIZE = 1024 * 1024;\n\nvoid print_checksum(std::ostream &os, uint8_t *md, size_t len);\n\nint main(int argc, char *argv[]) {\n\n  MPI_Init(&argc, &argv);\n\n  int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  if (rank == 0) {\n    if (argc < 3) {\n      std::cout << \"Usage: \" << argv[0] << \" <input_file> <output_file>\"\n                << std::endl;\n      MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n  }\n    fs::path input_path = argv[1];\n    fs::path output_path = argv[2];\n\n    auto total_begin_time = std::chrono::high_resolution_clock::now();\n\n    auto file_size = fs::file_size(input_path);\n    std::cout << input_path << \" size: \" << file_size << std::endl;\n    int num_block = (file_size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n    int proc_num_block = num_block / 8;\n    auto len = file_size / 8;\n\n    auto begin_time = std::chrono::high_resolution_clock::now();\n    \n    EVP_MD *sha512 = EVP_MD_fetch(nullptr, \"SHA512\", nullptr);\n    EVP_MD_CTX *ctx[proc_num_block];\n    for(int i=0; i<proc_num_block; i++)\n    {\n      ctx[i]=EVP_MD_CTX_new();\n      EVP_DigestInit_ex(ctx[i], sha512, nullptr);\n    }\n\n    uint8_t prev_md[SHA512_DIGEST_LENGTH];\n    std::ifstream istrm(input_path, std::ios::binary);\n    uint8_t buffer_temp[BLOCK_SIZE];\n    for (int i = 0; i < proc_num_block; i++) {\n      istrm.seekg(rank*len + i*BLOCK_SIZE);\n      istrm.read(reinterpret_cast<char *>(buffer_temp),BLOCK_SIZE);\n        //IO和CPU流水线\n      EVP_DigestUpdate(ctx[i], buffer_temp, BLOCK_SIZE);\n    }\n\n    if(rank==0){\n      SHA512(nullptr, 0, prev_md);\n      for(int i=0;i<proc_num_block;i++)\n      {\n        EVP_DigestUpdate(ctx[i], prev_md, SHA512_DIGEST_LENGTH);\n        unsigned int len_ex = 0;\n        EVP_DigestFinal_ex(ctx[i], prev_md, &len_ex);\n        EVP_MD_CTX_free(ctx[i]);\n      }\n      MPI_Send(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, 1, 0, MPI_COMM_WORLD);\n    }else if(rank>0 && rank<7)\n    {\n      MPI_Recv(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int i=0;i<proc_num_block;i++)\n      {\n        EVP_DigestUpdate(ctx[i], prev_md, SHA512_DIGEST_LENGTH);\n        unsigned int len_ex = 0;\n        EVP_DigestFinal_ex(ctx[i], prev_md, &len_ex);\n        EVP_MD_CTX_free(ctx[i]);\n      }\n      MPI_Send(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, rank + 1, 0, MPI_COMM_WORLD);      \n    }else if(rank==7){\n      MPI_Recv(prev_md, SHA512_DIGEST_LENGTH, MPI_UINT8_T, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int i=0;i<proc_num_block;i++)\n      {\n        EVP_DigestUpdate(ctx[i], prev_md, SHA512_DIGEST_LENGTH);\n        unsigned int len_ex = 0;\n        EVP_DigestFinal_ex(ctx[i], prev_md, &len_ex);\n        EVP_MD_CTX_free(ctx[i]);\n      }\n    }\n\n    EVP_MD_free(sha512);\n  \n  if(rank==7)\n  {\n    auto end_time = std::chrono::high_resolution_clock::now();\n\n    // print debug information\n    std::cout << \"checksum: \";\n    print_checksum(std::cout, prev_md, SHA512_DIGEST_LENGTH);\n    std::cout << std::endl;\n\n    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(\n        end_time - begin_time);\n\n    std::cout << \"checksum time cost: \" << std::dec << duration.count() << \"ms\"\n              << std::endl;\n\n    // write checksum to output file\n    std::ofstream output_file(output_path);\n\n    print_checksum(output_file, prev_md, SHA512_DIGEST_LENGTH);\n  \n    auto total_end_time = std::chrono::high_resolution_clock::now();\n\n    auto total_duration = std::chrono::duration_cast<std::chrono::milliseconds>(\n        total_end_time - total_begin_time);\n\n    std::cout << \"total time cost: \" << total_duration.count() << \"ms\"\n              << std::endl;\n  \n  }\n  MPI_Finalize();\n  return 0;\n}\n\nvoid print_checksum(std::ostream &os, uint8_t *md, size_t len) {\n  for (int i = 0; i < len; i++) {\n    os << std::setw(2) << std::setfill('0') << std::hex\n       << static_cast<int>(md[i]);\n  }\n}\n```\n\n速度得到了非常显著的提升，get满分\n\n## H.矩阵乘法\n\n [矩阵乘法.mhtml](矩阵乘法.mhtml) \n\n经典题目，同样的配方，用AVX512+Openmp+分块计算基本就能有不错的效果，不过我也只拿到了73/100分（菜就该多练），直接把printf去掉了都拿不到高分┭┮﹏┭┮\n\n```cpp\n#include <iostream>\n#include <chrono>\n#include <immintrin.h>\n#include <omp.h>\n#include <cmath>\n#define BLOCKSIZE 128\n#define AVX_F_CAPACITY 8\n\nvoid mul(double* a, double* b, double* c, uint64_t n1, uint64_t n2, uint64_t n3) {\n#pragma omp parallel for\n    for (uint64_t i = 0; i < n1; i+=BLOCKSIZE) {\n        for (uint64_t j = 0; j < n2; j+=BLOCKSIZE) {\n            for (uint64_t k = 0; k < n3; k+=BLOCKSIZE) {\n\n                for(uint64_t ii=i; ii<i+BLOCKSIZE; ii+=AVX_F_CAPACITY)\n                {\n                    for(uint64_t kk=k; kk<k+BLOCKSIZE; kk+=16)\n                    {\n                        __m512d vc0,vc1,vc2,vc3,vc4,vc5,vc6,vc7,vc8,vc9,vc10,vc11,vc12,vc13,vc14,vc15,vb,vb1;\n                        vc0 = _mm512_load_pd(&c[ii*n3+kk]);\n                        vc8 = _mm512_load_pd(&c[ii*n3+kk+8]);\n                        vc1 = _mm512_load_pd(&c[(ii+1)*n3+kk]);\n                        vc9 = _mm512_load_pd(&c[(ii+1)*n3+kk+8]);\n                        vc2 = _mm512_load_pd(&c[(ii+2)*n3+kk]);\n                        vc10 = _mm512_load_pd(&c[(ii+2)*n3+kk+8]);\n                        vc3 = _mm512_load_pd(&c[(ii+3)*n3+kk]);\n                        vc11 = _mm512_load_pd(&c[(ii+3)*n3+kk+8]);\n                        vc4 = _mm512_load_pd(&c[(ii+4)*n3+kk]);\n                        vc12 = _mm512_load_pd(&c[(ii+4)*n3+kk+8]);\n                        vc5 = _mm512_load_pd(&c[(ii+5)*n3+kk]);\n                        vc13 = _mm512_load_pd(&c[(ii+5)*n3+kk+8]);\n                        vc6 = _mm512_load_pd(&c[(ii+6)*n3+kk]);\n                        vc14 = _mm512_load_pd(&c[(ii+6)*n3+kk+8]);\n                        vc7 = _mm512_load_pd(&c[(ii+7)*n3+kk]);\n                        vc15 = _mm512_load_pd(&c[(ii+7)*n3+kk+8]);\n\n                        for(uint64_t jj=j; jj<j+BLOCKSIZE; jj+=AVX_F_CAPACITY)\n                        {\n                            vb=_mm512_load_pd(&b[jj*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[jj*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj]),vb,vc7);\n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj]),vb1,vc15);\n                        \n\n                            vb=_mm512_load_pd(&b[(jj+1)*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[(jj+1)*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+1]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+1]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+1]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+1]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+1]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+1]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+1]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+1]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+1]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+1]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+1]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+1]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+1]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+1]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+1]),vb,vc7);\n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+1]),vb1,vc15);\n\n                            vb=_mm512_load_pd(&b[(jj+2)*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[(jj+2)*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+2]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+2]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+2]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+2]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+2]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+2]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+2]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+2]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+2]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+2]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+2]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+2]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+2]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+2]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+2]),vb,vc7);\n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+2]),vb1,vc15);\n\n                            vb=_mm512_load_pd(&b[(jj+3)*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[(jj+3)*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+3]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+3]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+3]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+3]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+3]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+3]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+3]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+3]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+3]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+3]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+3]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+3]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+3]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+3]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+3]),vb,vc7);\n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+3]),vb1,vc15);\n                            \n                            vb=_mm512_load_pd(&b[(jj+4)*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[(jj+4)*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+4]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+4]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+4]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+4]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+4]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+4]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+4]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+4]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+4]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+4]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+4]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+4]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+4]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+4]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+4]),vb,vc7);\n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+4]),vb1,vc15);\n\n                            vb=_mm512_load_pd(&b[(jj+5)*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[(jj+5)*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+5]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+5]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+5]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+5]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+5]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+5]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+5]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+5]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+5]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+5]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+5]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+5]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+5]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+5]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+5]),vb,vc7);\n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+5]),vb1,vc15);\n                            \n\n                            vb=_mm512_load_pd(&b[(jj+6)*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[(jj+6)*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+6]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+6]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+6]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+6]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+6]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+6]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+6]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+6]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+6]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+6]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+6]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+6]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+6]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+6]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+6]),vb,vc7);\n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+6]),vb1,vc15);\n\n                            vb=_mm512_load_pd(&b[(jj+7)*n3 + kk]);\n                            vb1=_mm512_load_pd(&b[(jj+7)*n3 + kk+8]);\n                            vc0 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+7]),vb,vc0);\n                            vc8 = _mm512_fmadd_pd(_mm512_set1_pd(a[ii*n2+jj+7]),vb1,vc8);\n                            vc1 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+7]),vb,vc1);\n                            vc9 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+1)*n2+jj+7]),vb1,vc9);\n                            vc2 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+7]),vb,vc2);\n                            vc10 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+2)*n2+jj+7]),vb1,vc10);\n                            vc3 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+7]),vb,vc3);\n                            vc11 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+3)*n2+jj+7]),vb1,vc11);\n                            vc4 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+7]),vb,vc4);\n                            vc12 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+4)*n2+jj+7]),vb1,vc12);\n                            vc5 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+7]),vb,vc5);\n                            vc13 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+5)*n2+jj+7]),vb1,vc13);\n                            vc6 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+7]),vb,vc6);\n                            vc14 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+6)*n2+jj+7]),vb1,vc14);\n                            vc7 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+7]),vb,vc7);                            \n                            vc15 = _mm512_fmadd_pd(_mm512_set1_pd(a[(ii+7)*n2+jj+7]),vb1,vc15);\n                        }\n                        _mm512_store_pd(&c[ii*n3 + kk],vc0);\n                        _mm512_store_pd(&c[ii*n3 + kk+8],vc8);\n                        _mm512_store_pd(&c[(ii+1)*n3 + kk],vc1);\n                        _mm512_store_pd(&c[(ii+1)*n3 + kk+8],vc9);\n                        _mm512_store_pd(&c[(ii+2)*n3 + kk],vc2);\n                        _mm512_store_pd(&c[(ii+2)*n3 + kk+8],vc10);\n                        _mm512_store_pd(&c[(ii+3)*n3 + kk],vc3);\n                        _mm512_store_pd(&c[(ii+3)*n3 + kk+8],vc11);\n                        _mm512_store_pd(&c[(ii+4)*n3 + kk],vc4);\n                        _mm512_store_pd(&c[(ii+4)*n3 + kk+8],vc12);\n                        _mm512_store_pd(&c[(ii+5)*n3 + kk],vc5);\n                        _mm512_store_pd(&c[(ii+5)*n3 + kk+8],vc13);\n                        _mm512_store_pd(&c[(ii+6)*n3 + kk],vc6);\n                        _mm512_store_pd(&c[(ii+6)*n3 + kk+8],vc14);\n                        _mm512_store_pd(&c[(ii+7)*n3 + kk],vc7);\n                        _mm512_store_pd(&c[(ii+7)*n3 + kk+8],vc15);\n                    }\n                }\n            \n            }\n        }\n    }\n}\nint main() {\n uint64_t n1, n2, n3;\n FILE* fi;\n\n fi = fopen(\"conf.data\", \"rb\");\n fread(&n1, 1, 8, fi);\n fread(&n2, 1, 8, fi);\n fread(&n3, 1, 8, fi);\n\n double* a = (double*)_mm_malloc(n1 * n2 * 8,64);\n double* b = (double*)_mm_malloc(n2 * n3 * 8,64);\n double* c = (double*)_mm_malloc(n1 * n3 * 8,64);\n\n fread(a, 1, n1 * n2 * 8, fi);\n fread(b, 1, n2 * n3 * 8, fi);\n fclose(fi);\n\n for (uint64_t i = 0; i < n1; i++) {\n  for (uint64_t k = 0; k < n3; k++) {\n   c[i * n3 + k] = 0;\n  }\n }\n\n\n auto t1 = std::chrono::steady_clock::now();\n mul(a, b, c, n1, n2, n3);\n auto t2 = std::chrono::steady_clock::now();\n int d1 = std::chrono::duration_cast<std::chrono::milliseconds>(t2 - t1).count();\n printf(\"%d\\n\", d1);\n// printf(\"c:%.4lf\\n\",c[202]);\n\n fi = fopen(\"out.data\", \"wb\");\n fwrite(c, 1, n1 * n3 * 8, fi);\n fclose(fi);\n\n return 0;\n}\n```\n\n\n\n## I.logistic 方程\n\n [Logistic方程.mhtml](Logistic方程.mhtml) \n\n这题卡了一下午，分数一直是20多分，实在看不下去\n\n一开始只是对it、itv函数进行简单的AVX和openmp处理，出来的结果实在不让人满意，后来经过对函数结构的观察，发现两个循环可以换位置，于是交换了一下，再在内部开openmp，确实还是差点意思，想到这道题反复强调向量化，就尝试将迭代次数减少一部分，放进内部循环，此时发现效果还可以。代码如下：\n\n```cpp\n#define N 8\ndouble count_itv(double r, double x) {\n    return r * x * (1.0 - x);\n}\n\nvoid itv(double r, double* x, int64_t n, int64_t itn) {\n    #pragma omp parallel\n    {\n        for (int64_t j = 0; j < itn/N; j++) {\n            #pragma omp for\n            for (int64_t i = 0; i < n; i++) {\n                for (int64_t k = 0; k <N; k++) {\n                    x[i] = count_itv(r, x[i]);\n                }\n            }\n        }\n    }\n}\n```\n\n数字16是经过反复尝试，调整出来的最好结果，因为itn=32768，所以必须选择可以除尽的数\n\n![image-20240124010147362](HPCGame记录/image-20240124010147362.png)\n\n还差4分，难受，改了一下手动向量化，代码如下：\n\n```cpp\n    __m512d avx_one = _mm512_set1_pd(1.0);\n    __m512d avx_r = _mm512_set1_pd(r);\n    #pragma omp parallel\n    {\n        for (short i=0; i<itn/N; ++i) {\n            #pragma omp for\n            for (int64_t j=0; j<n; j+=8) {\n                __m512d avx_x = _mm512_load_pd(&x[j]);\n                for (short k=0; k<N; ++k) {\n                    __m512d avx_tmp = _mm512_mul_pd(avx_r, avx_x);\n                    avx_x = _mm512_mul_pd(avx_tmp, _mm512_sub_pd(avx_one, avx_x));\n                }\n                _mm512_store_pd(&x[j], avx_x);\n            }\n        }\n    }\n```\n\n![image-20240124013348557](HPCGame记录/image-20240124013348557.png)\n\n额(⊙o⊙)…毁灭吧，我累了\n\n后期突然注意到，openmp开错了，这道题还是openmp+向量化，但是我神奇般的过了...卡了个bug\n\n来自一位学长的**修正：**\n\n```cpp\n#include <iostream>\n#include <chrono>\n#include <omp.h>\n#include <immintrin.h>\n\n// Define macro for aligning memory to 64 bytes for AVX-512\n#define ALIGNMENT 64\n\ndouble it(double r, double x, int64_t itn) {\n    for (int64_t i = 0; i < itn; i++) {\n        x = r * x * (1.0 - x);\n    }\n    return x;\n}\n\nvoid itv(double r, double* x, int64_t n, int64_t itn) {\n    __builtin_assume_aligned(x, 64);\n    __m512d r_vec=_mm512_set1_pd(r);\n    __m512d vec_1=_mm512_set1_pd(1.0);\n    #pragma omp parallel for collapse(1) \n    #pragma vector aligned\n    for (int64_t i = 0; i < n; i += 64) {\n        __m512d x_vec1 = _mm512_loadu_pd(&x[i]);\n        __m512d x_vec2 = _mm512_loadu_pd(&x[i+8]);\n        __m512d x_vec3 = _mm512_loadu_pd(&x[i+16]);\n        __m512d x_vec4 = _mm512_loadu_pd(&x[i+24]);\n        __m512d x_vec5 = _mm512_loadu_pd(&x[i+32]);\n        __m512d x_vec6 = _mm512_loadu_pd(&x[i+40]);\n        __m512d x_vec7 = _mm512_loadu_pd(&x[i+48]);\n        __m512d x_vec8 = _mm512_loadu_pd(&x[i+56]);\n        for (int64_t j = 0; j < itn; j++) {\n            x_vec1 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec1),\n                                   _mm512_sub_pd(vec_1, x_vec1));\n            x_vec2 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec2),\n                       \t\t\t   _mm512_sub_pd(vec_1, x_vec2));\n            x_vec3 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec3),\n                       \t\t\t   _mm512_sub_pd(vec_1, x_vec3));\n            x_vec4 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec4),\n                       \t\t\t   _mm512_sub_pd(vec_1, x_vec4));\n            x_vec5 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec5),\n                       \t\t\t   _mm512_sub_pd(vec_1, x_vec5));\n            x_vec6 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec6),\n                       \t\t\t   _mm512_sub_pd(vec_1, x_vec6));\n            x_vec7 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec7),\n                       \t\t\t   _mm512_sub_pd(vec_1, x_vec7));\n            x_vec8 = _mm512_mul_pd(_mm512_mul_pd(r_vec, x_vec8),\n           \t\t\t   \t\t\t   _mm512_sub_pd(vec_1, x_vec8));\n        }\n        _mm512_storeu_pd(&x[i], x_vec1);\n        _mm512_storeu_pd(&x[i+8], x_vec2);\n        _mm512_storeu_pd(&x[i+16], x_vec3);\n        _mm512_storeu_pd(&x[i+24], x_vec4);\n        _mm512_storeu_pd(&x[i+32], x_vec5);\n        _mm512_storeu_pd(&x[i+40], x_vec6);\n        _mm512_storeu_pd(&x[i+48], x_vec7);\n        _mm512_storeu_pd(&x[i+56], x_vec8);\n    }\n}\n\nint main(){\n    FILE* fi;\n    fi = fopen(\"conf.data\", \"rb\");\n\n    int64_t itn;\n    double r;\n    int64_t n;\n    double* x;\n\n    fread(&itn, 1, 8, fi);\n    fread(&r, 1, 8, fi);\n    fread(&n, 1, 8, fi);\n\n    // Allocate aligned memory for better AVX-512 performance\n    x = (double*)_mm_malloc(n * 8, ALIGNMENT);\n\n    fread(x, 1, n * 8, fi);\n    fclose(fi);\n\n    auto t1 = std::chrono::steady_clock::now();\n\n    // Use OpenMP for parallelization and AVX-512 for vectorization\n    itv(r, x, n, itn);\n\n    auto t2 = std::chrono::steady_clock::now();\n    int d1 = std::chrono::duration_cast<std::chrono::milliseconds>(t2 - t1).count();\n    printf(\"%d\\n\", d1);\n\n    fi = fopen(\"out.data\", \"wb\");\n    fwrite(x, 1, n * 8, fi);\n    fclose(fi);\n\n    // Free aligned memory\n    _mm_free(x);\n\n    return 0;\n}\n```\n\n\n\n## J.H-66\n\n [H-66.mhtml](H-66.mhtml) \n\n先用了gprof进行了热点分析，结果如下：\n\n![image-20240124102831082](HPCGame记录/image-20240124102831082.png)\n\n可以看到，大部分时间都是花费在了getsp函数上，act函数也是个小热点，\n\n先拿getsp开刀，\n\n1月29日：“遗憾，玩不动这道题”\n\n## L.洪水困兽\n\n [洪水困兽.mhtml](洪水困兽.mhtml) \n\n签到送分题，OpenMP直接过\n\n```cpp\n#include <array>\n#include <fstream>\n#include <iostream>\n#include <omp.h>\n#include <vector>\n#include <cmath>\n#include <tuple>\n\nusing std::vector, std::array, std::tuple, std::string;\n\nvoid particle2grid(int resolution, int numparticle,\n                   const vector<double> &particle_position,\n                   const vector<double> &particle_velocity,\n                   vector<double> &velocityu, vector<double> &velocityv,\n                   vector<double> &weightu, vector<double> &weightv) {\n    double grid_spacing = 1.0 / resolution;\n    double inv_grid_spacing = 1.0 / grid_spacing;\n    auto get_frac = [&inv_grid_spacing](double x, double y) {\n        int xidx = floor(x * inv_grid_spacing);\n        int yidx = floor(y * inv_grid_spacing);\n        double fracx = x * inv_grid_spacing - xidx;\n        double fracy = y * inv_grid_spacing - yidx;\n        return tuple(array<int, 2>{xidx, yidx},\n                     array<double, 4>{fracx * fracy, (1 - fracx) * fracy,\n                                      fracx * (1 - fracy),\n                                      (1 - fracx) * (1 - fracy)});\n    };\n\n    #pragma omp parallel for\n    for (int i = 0; i < numparticle; i++) {\n        array<int, 4> offsetx = {0, 1, 0, 1};\n        array<int, 4> offsety = {0, 0, 1, 1};\n\n        auto [idxu, fracu] =\n            get_frac(particle_position[i * 2 + 0],\n                     particle_position[i * 2 + 1] - 0.5 * grid_spacing);\n        auto [idxv, fracv] =\n            get_frac(particle_position[i * 2 + 0] - 0.5 * grid_spacing,\n                     particle_position[i * 2 + 1]);\n\n        for (int j = 0; j < 4; j++) {\n            int tmpidx_u = (idxu[0] + offsetx[j]) * resolution + (idxu[1] + offsety[j]);\n            int tmpidx_v = (idxv[0] + offsetx[j]) * (resolution + 1) + (idxv[1] + offsety[j]);\n//关键点就在这里,知道有这个操作就能签到成功，但看到有人手写锁好像也成功了\n            #pragma omp atomic\n            velocityu[tmpidx_u] += particle_velocity[i * 2 + 0] * fracu[j];\n\n            #pragma omp atomic\n            weightu[tmpidx_u] += fracu[j];\n\n            #pragma omp atomic\n            velocityv[tmpidx_v] += particle_velocity[i * 2 + 1] * fracv[j];\n\n            #pragma omp atomic\n            weightv[tmpidx_v] += fracv[j];\n        }\n    }\n}\n\nint main(int argc, char *argv[]) {\n    if (argc < 2) {\n        printf(\"Usage: %s inputfile\\n\", argv[0]);\n        return -1;\n    }\n\n    string inputfile(argv[1]);\n    std::ifstream fin(inputfile, std::ios::binary);\n    if (!fin) {\n        printf(\"Error opening file\");\n        return -1;\n    }\n    \n    int resolution;\n    int numparticle;\n    vector<double> particle_position;\n    vector<double> particle_velocity;\n\n    fin.read((char *)(&resolution), sizeof(int));\n    fin.read((char *)(&numparticle), sizeof(int));\n    \n    particle_position.resize(numparticle * 2);\n    particle_velocity.resize(numparticle * 2);\n    \n    printf(\"resolution: %d\\n\", resolution);\n    printf(\"numparticle: %d\\n\", numparticle);\n    \n    fin.read((char *)(particle_position.data()),\n             sizeof(double) * particle_position.size());\n    fin.read((char *)(particle_velocity.data()),\n             sizeof(double) * particle_velocity.size());\n\n    vector<double> velocityu((resolution + 1) * resolution, 0.0);\n    vector<double> velocityv((resolution + 1) * resolution, 0.0);\n    vector<double> weightu((resolution + 1) * resolution, 0.0);\n    vector<double> weightv((resolution + 1) * resolution, 0.0);\n\n\n    string outputfile;\n\n    particle2grid(resolution, numparticle, particle_position,\n                    particle_velocity, velocityu, velocityv, weightu,\n                    weightv);\n    outputfile = \"output.dat\";\n\n    std::ofstream fout(outputfile, std::ios::binary);\n    if (!fout) {\n        printf(\"Error output file\");\n        return -1;\n    }\n    fout.write((char *)(&resolution), sizeof(int));\n    fout.write(reinterpret_cast<char *>(velocityu.data()),\n               sizeof(double) * velocityu.size());\n    fout.write(reinterpret_cast<char *>(velocityv.data()),\n               sizeof(double) * velocityv.size());\n    fout.write(reinterpret_cast<char *>(weightu.data()),\n               sizeof(double) * weightu.size());\n    fout.write(reinterpret_cast<char *>(weightv.data()),\n               sizeof(double) * weightv.size());\n\n    return 0;\n}\n```\n\n## 其余题：\n\n [光之游戏.mhtml](光之游戏.mhtml) \n\n [3D生命游戏.mhtml](3D生命游戏.mhtml) \n\nRISC-V的题没有放\n","tags":["比赛"]},{"title":"计算机组成原理","url":"/2024/01/22/计算机组成原理/","content":"\n# 一、计算机系统概论\n\n## 1.1计算机系统简介\n\n**物联网：**把<font color=red>传感器</font>嵌入和装备到各种物体中，并且被普遍连接\n\n计算机系统由<u>**硬件**</u>和<u>**软件**</u>组成\n\n- 硬件：计算机的实体，如主机、外设等\n\n- 软件：由具有各类特殊功能的程序组成 \n\n  ```\n  系统软件：用来管理整个计算机系统，如语言处理程序、操作系统、服务性程序、数据库管理系统、网络软件(如tcp/ip协议的模块)\n  应用软件：按任务需要贬值的各种程序\n  ```\n\n## 1.2计算机硬件基本组成\n\n### 1.2.1冯诺依曼机\n\n第一台计算机ENIAC(埃尼亚克)通过手动接线的方式进行控制计算，效率受到接线的限制\n\n冯诺依曼提出“**存储程序**”：将指令以二进制代码的形式实现输入到计算机的主存储器，然后从首地址开始按序执行指令\n\n**早期冯诺依曼体系结构：**\n\n![image-20240122152233293](计算机组成原理/image-20240122152233293.png)\n\n<img src=\"计算机组成原理/image-20240122152116407.png\" alt=\"image-20240122152116407\" style=\"zoom:80%;\" />\n\n步骤：\n\n```\n数据或程序先进入输入设备，将信息转换为机器能识别的形式\n数据通过运算器进行中转到存储器中\n控制器读入存储器中的运算指令（加、乘等）控制运算器做算术运算或逻辑运算\n经过运算后的数据通过输出设备转换为人们能识别的结果\n```\n\n冯诺依曼计算机的特点：\n\n```\n计算机由五大部分组成：输入设备、输出设备、控制器、运算器、存储器\n指令和数据以同等地位存于存储器，可按址寻访\n指令和数据用二进制表示\n指令由操作码和地址码组成\n存储程序：提前把指令和数据存储到存储器中\n以运算器为中心（输入/输出与存储器之间的数据传输通过运算器完成）\n```\n\n### 1.2.2现代计算机的结构\n\n以<font color=red>**存储器**</font>为中心\n\n<img src=\"计算机组成原理/image-20240122154625927.png\" alt=\"image-20240122154625927\" style=\"zoom:80%;\" />\n\nCPU=运算器+控制器\n\n![image-20240122154923848](计算机组成原理/image-20240122154923848.png)\n\n<img src=\"计算机组成原理/image-20240122192046146.png\" alt=\"image-20240122192046146\" style=\"zoom:80%;\" />\n\n**主存：**RAM运行内存\n\n**辅存：**ROM存储大小\n\n### 1.2.3主存储器\n\n![image-20240122210202202](计算机组成原理/image-20240122210202202.png)\n\n**读取：**CPU将地址传入MAR，从MDR获得数据，控制器控制主存储器执行\"读\"操作\n\n**写入：**CPU将地址传入MAR，数据传入MDR，控制器控制主存储器进行\"写\"操作\n\n**【存储体】：**\n\n数据都在存储体内按<font color=red>地址</font>存储\n\n![image-20240122210906368](计算机组成原理/image-20240122210906368.png)\n\n存储单元：存储器最小的存储单位，是一串存放二进制代码的空间\n\n存储字（word）：存储单元中二进制代码的组合\n\n存储字长：存储单元中二进制代码的位数（通常为8bit的整数倍）\n\n存储元：用于存储二进制的的电子元件（电容），每个存储元可以存1bit\n\n因此，MAR位数可以反映存储单元的个数，MDR的位数等于存储字长\n","tags":["计算机基础"]}]